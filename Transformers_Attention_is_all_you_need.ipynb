{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers-Attention is all you need.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCSZbfqj/YZemBl61qydg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/Transformers/blob/main/Transformers_Attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2S7J_UP2933",
        "outputId": "0966c673-8841-4da4-cfe1-44e2a3c55d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.10.0+cu111)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer was the first transduction model relying entirely on self-attention to compute representations iof its input and output without using sequence aligned RNNs or Convolutions. (*from paper*)"
      ],
      "metadata": {
        "id": "-dnfB1R4DnQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy \n",
        "from torch.autograd import Variable\n",
        "\n",
        "#For Plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "x6fvp-Zo8_9g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\" A standard encoder-decoder architecture \"\"\"\n",
        "  def __init__(self, encoder, decoder,src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    #take in and process masked spource and target sequences\n",
        "    return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "  def decode(self,memory, src_mask, tgt, tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
      ],
      "metadata": {
        "id": "7l-o7KcVEmlz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  #Defining standard linear and softmax generation step\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.proj(x),dim=-1) "
      ],
      "metadata": {
        "id": "kpHpbTannShr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder - Decoder Stack \n",
        "### Encoder consists of 6 identical layers"
      ],
      "metadata": {
        "id": "sCnOwB5-zC7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N=6):\n",
        "  \"Creates N identical layers of the type\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "VbiNYmmpzLX9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  #Coding for encoder block\n",
        "  def __init__(self,layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,x, mask):\n",
        "    \"Pass input and mask through each layer by turn\"\n",
        "    for layer in self.layers:\n",
        "      x =  layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "WCxph80gzqfG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying residual layer and normalization layers"
      ],
      "metadata": {
        "id": "7q-_QOKE5AbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps=eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean= x.mean(-1, keepdim=True)\n",
        "    std= x.std(-1,keepdim=True)\n",
        "    return self.a_2*(x-mean)/(std+self.eps) +self.b_2"
      ],
      "metadata": {
        "id": "7WOrQ3V93KMi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  \"Residual connection added to Layer Norm\"\n",
        "  def __init__(sel,size, dropout):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x, sublayer):\n",
        "    \"Apply residual connections\"\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "nN7_ZEX6O3gC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Block :\n"
      ],
      "metadata": {
        "id": "v_ubErT1LG5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  \"Encoder is made of Self-Attention and feed forward network\"\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward =  feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout),2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"Encoder block\"\n",
        "    x = self.sublayer[0](x, lambda x:self.self_attn(x,x,x, mask))\n",
        "    return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "BcyelS_n7JFn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "EpNtnWjhVvgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"N layer Decoder with masking\"\n",
        "  def __init__(self, layer, N):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer, size)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    return self.norm(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "foJuPuvjVXCY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder has an additional layer which performs muli-head attention on the encoder's output. SO there are three layers in Decoder - Multi-headed masked attention layer, multi-headed attention on Encoder's input, and feed-forward layer.\n",
        "Also, note that there is a residual connection around each layer, follwoed by a LayerNorm."
      ],
      "metadata": {
        "id": "FJ-BfEwEVvv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  \"Decoder is made of self-attn, src-attn, and feed-forward layers\"\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super(DecoderLayer, self ).__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout),3)\n",
        "  \n",
        "  def forward(self,x,memory, src_mask, tgt_mask):\n",
        "    \"Decoder achitecture\"\n",
        "    m = memory\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x,m,m,src_mask))\n",
        "    return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "UMGoAFgpXB7A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to implement masking for Decoder inputs - to make sure that Decoder is not attending to subsequent input position than what it is processing right now."
      ],
      "metadata": {
        "id": "7_GgRbWsbipo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  \"MAsking subsequent positions\"\n",
        "  attn_shape = (1,size, size)\n",
        "  subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
        "  return torch.from_numpy(subsequent_mask) == 0"
      ],
      "metadata": {
        "id": "jqxAgxyka3nH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "qwLlw3PudDut",
        "outputId": "f4a2a6ad-aee6-446a-cf53-751d6fdc0d1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8f9cd3f850>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6ysO7h8ueCID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}